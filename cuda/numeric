//
// Created by egi on 1/26/20.
//

#ifndef LIBCUPP_PROPOSAL_NUMERIC_H
#define LIBCUPP_PROPOSAL_NUMERIC_H

#include "detail/__reduce"

namespace cuda {

template<typename T, typename... Rest>
struct is_any : std::false_type {};

template<typename T, typename First>
struct is_any<T, First> : std::is_same<T, First> {};

template<typename T, typename First, typename... Rest>
struct is_any<T, First, Rest...> : std::integral_constant<bool, std::is_same<T, First>::value || is_any<T, Rest...>::value>
{};

template <int required_cc>
constexpr bool check_compute_capability ()
{
#if defined(__CUDA_ARCH__)
  return __CUDA_ARCH__ >= required_cc;
#else
  return false;
#endif
}

/**
 * @brief Class for parallel reduse within warp
 *
 * Reduce uses a binary combining operator to compute a single aggregate from an array of elements. The
 * number of entrant threads must be equal to warpSize. After reduce, the result is the same in all threads.
 * The default binary combining operator is the sum.
 *
 * @tparam _Tp Reduced data type
 * @tparam _ReducePolicy Policy for warp threads data exchange
 */
template<
    typename _Tp,
    typename _ReducePolicy = typename std::conditional<
        std::integral_constant<bool, is_any<_Tp,
                                            int,
                                            long,
                                            long long,
                                            unsigned int,
                                            unsigned long,
                                            unsigned long long,
                                            float,
                                            double>::value
                                  && check_compute_capability<300>()>::value,
                               detail::__warp_shfl_reduce<_Tp>,
                               detail::__warp_shrd_reduce<_Tp>>::type>
class warp_reduce : public _ReducePolicy {
public:
  using _ReducePolicy::_ReducePolicy;

  /**
   * @tparam _BinaryOperation Binary combining function object thata will be applied in unspecified order.
   *                          The behaviour is undefined if _BinaryOperation modifies any element.
   */
  template<typename _RandomAccessIterator, typename _BinaryOperation = detail::__default_reduce_binary_op<_Tp>>
  __device__ inline _Tp
  operator ()(_RandomAccessIterator __first, _RandomAccessIterator __last, _Tp __init, _BinaryOperation __binary_op = {})
  {
    for (__first += detail::lane_id(); __first < __last; __first += warpSize)
      __init = __binary_op (__init, *__first);
    return __reduce_value (__init, __binary_op);
  }

  /**
   * @tparam _BinaryOperation Binary combining function object thata will be applied in unspecified order.
   *                          The behaviour is undefined if _BinaryOperation modifies any element.
   */
  template<typename _BinaryOperation = detail::__default_reduce_binary_op<_Tp>>
  __device__ inline _Tp
  operator ()(_Tp __val, _BinaryOperation __binary_op = {})
  {
    return __reduce_value (__val, __binary_op);
  }
};

template <typename _Tp, bool _UseSharedMemory>
class __warp_reduce_default_dependency_injector;

template <typename _Tp>
class __warp_reduce_default_dependency_injector<_Tp, false>
{
public:
  static __device__ warp_reduce<_Tp> construct_warp_reduce_object (_Tp *) { return {}; }
};

template <typename _Tp>
class __warp_reduce_default_dependency_injector<_Tp, true>
{
public:
  static __device__ warp_reduce<_Tp> construct_warp_reduce_object (_Tp *__warp_shared) { return {__warp_shared}; }
};


template <int warp_size=32>
class __default_indexing_policy_1D {
public:
  static __device__ int get_blck_id () { return blockIdx.x; }
  static __device__ int get_thrd_id () { return threadIdx.x; }
  static __device__ int get_lane_id () { return threadIdx.x % warp_size; }
  static __device__ int get_warp_id () { return threadIdx.x / warp_size; }

  static __device__ int get_n_thrd_in_block () { return blockDim.x; }
  static __device__ int get_n_warp_in_block () { return get_n_thrd_in_block() / warp_size; }

};

/**
 * @brief Class for parallel reduse within threads block
 *
 * Reduce uses a binary combining operator to compute a single aggregate from an array of elements.
 * The threads number must be multiple of warpSize. After reduce, the result is the same in all threads.
 *
 * @tparam _Tp Reduced data type
 * @tparam _ReducePolicy Policy for warp threads data exchange
 */
template<
    typename _Tp,
    typename _IndexingPolicy=__default_indexing_policy_1D<32>,
    typename _WarpReduceDependencyInjectionPolicy=__warp_reduce_default_dependency_injector<_Tp, warp_reduce<_Tp>::use_shared>>
class block_reduce {
  _Tp *block_shared_workspace;

public:
  explicit __device__ block_reduce (_Tp *block_shared_workspace_arg) : block_shared_workspace (block_shared_workspace_arg) { }

  template<typename _BinaryOperation = detail::__default_reduce_binary_op<_Tp>>
  __device__ inline _Tp
  operator ()(_Tp __val, _BinaryOperation __binary_op = {})
  {
    const int tid = _IndexingPolicy::get_thrd_id ();
    const int lid = _IndexingPolicy::get_lane_id ();
    const int wid = _IndexingPolicy::get_warp_id ();

    auto __warp_reduce = _WarpReduceDependencyInjectionPolicy::construct_warp_reduce_object (block_shared_workspace + wid * warpSize);
    auto __warp_result = __warp_reduce (__val, __binary_op);
    __syncthreads ();

    if (lid == 0)
      block_shared_workspace[wid] = __warp_result;
    __syncthreads ();

    for (int s = _IndexingPolicy::get_n_warp_in_block () / 2; s > 0; s >>= 1) {
      if (tid < s) {
        block_shared_workspace[tid] = __binary_op(block_shared_workspace[tid], block_shared_workspace[tid + s]);
      }
      __syncthreads ();
    }

    _Tp result = block_shared_workspace[0];
    __syncthreads ();
    return result;
  }
};

}

#endif //LIBCUPP_PROPOSAL_NUMERIC_H
